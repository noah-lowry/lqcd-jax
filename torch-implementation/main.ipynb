{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import numpy as np\n",
    "from sun_utils import SUGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = SUGenerator(3, module=\"torch\", dtype=torch.complex64, device=\"cuda:0\")\n",
    "\n",
    "def coef_to_lie_group(coef):\n",
    "    # Ensure coef has the correct dtype before contraction.\n",
    "    coef = coef.to(dtype=gen.generators.dtype)\n",
    "    # Contract with the generators (note: same Einstein summation as in JAX)\n",
    "    su = torch.einsum(\"...N,Nij->...ij\", coef, gen.generators)\n",
    "    # Use PyTorch’s matrix exponential for complex matrices.\n",
    "    SU_N = torch.linalg.matrix_exp(1j * su)\n",
    "    return SU_N\n",
    "\n",
    "def wilson_action(field, beta):\n",
    "    N = field.shape[-1]\n",
    "\n",
    "    def plaquette(mu, nu):\n",
    "        # Select the mu and nu directions (assume field shape: [..., 4, N, N])\n",
    "        U_mu = field[..., mu, :, :]\n",
    "        U_nu = field[..., nu, :, :]\n",
    "        # Roll along the corresponding axis\n",
    "        U_mu_shifted = torch.roll(U_mu, shifts=-1, dims=nu)\n",
    "        U_nu_shifted = torch.roll(U_nu, shifts=-1, dims=mu)\n",
    "        # Compute the conjugate transpose as .transpose(-2, -1).conj()\n",
    "        U_mu_shifted_H = U_mu_shifted.transpose(-2, -1).conj()\n",
    "        U_nu_H = U_nu.transpose(-2, -1).conj()\n",
    "        # Compute the trace contraction (like the JAX einsum)\n",
    "        Re_Tr_Plaquettes = torch.einsum(\"...AB,...BC,...CD,...DA->...\", \n",
    "                                         U_mu, U_nu_shifted, U_mu_shifted_H, U_nu_H).real\n",
    "        return 1 - Re_Tr_Plaquettes / N\n",
    "\n",
    "    S = 0\n",
    "    for mu in range(4):\n",
    "        for nu in range(mu+1, 4):\n",
    "            S += torch.sum(plaquette(mu, nu))\n",
    "    return beta * S\n",
    "\n",
    "def tree_level_improved_action(field, beta):\n",
    "    def P(mu, nu):\n",
    "        U_mu = field[:, mu, :, :]\n",
    "        U_nu = field[:, nu, :, :]\n",
    "        rolled1 = torch.roll(U_nu, shifts=-1, dims=mu)\n",
    "        # Roll U_mu in the nu direction and take conjugate transpose\n",
    "        rolled2 = torch.roll(U_mu, shifts=-1, dims=nu).transpose(-2, -1).conj()\n",
    "        U_nu_H = U_nu.transpose(-2, -1).conj()\n",
    "        return torch.einsum(\"...AB,...BC,...CD,...DA->...\", \n",
    "                             U_mu, rolled1, rolled2, U_nu_H).real\n",
    "\n",
    "    def R(mu, nu):\n",
    "        U_mu = field[:, mu, :, :]\n",
    "        U_nu = field[:, nu, :, :]\n",
    "        R1 = torch.einsum(\n",
    "            \"...AB,...BC,...CD,...DE,...EF,...FA->...\",\n",
    "            U_mu,\n",
    "            torch.roll(U_nu, shifts=-1, dims=mu),\n",
    "            torch.roll(U_nu, shifts=(-1, -1), dims=(mu, nu)),\n",
    "            torch.roll(U_mu, shifts=-2, dims=nu).transpose(-2, -1).conj(),\n",
    "            torch.roll(U_nu, shifts=-1, dims=nu).transpose(-2, -1).conj(),\n",
    "            U_nu.transpose(-2, -1).conj()\n",
    "        ).real\n",
    "        R2 = torch.einsum(\n",
    "            \"...AB,...BC,...CD,...DE,...EF,...FA->...\",\n",
    "            U_mu,\n",
    "            torch.roll(U_mu, shifts=-1, dims=mu),\n",
    "            torch.roll(U_nu, shifts=-2, dims=mu),\n",
    "            torch.roll(U_mu, shifts=(-1, -1), dims=(mu, nu)).transpose(-2, -1).conj(),\n",
    "            torch.roll(U_mu, shifts=-1, dims=nu).transpose(-2, -1).conj(),\n",
    "            U_nu.transpose(-2, -1).conj()\n",
    "        ).real\n",
    "        return R1 + R2\n",
    "\n",
    "    # Compute the average plaquette value to define u0.\n",
    "    P_vals = [P(mu, nu) for mu in range(4) for nu in range(4) if mu != nu]\n",
    "    P_mean = torch.stack(P_vals).mean() / 3.0\n",
    "    u0_sqr = torch.pow(P_mean, 1/2)\n",
    "    print(f\"u0^2 = {u0_sqr}\")\n",
    "\n",
    "    S = 0\n",
    "    for mu in range(4):\n",
    "        for nu in range(mu+1, 4):\n",
    "            S += (5 * beta) * (1 - P(mu, nu).sum() / 3) - (beta / (4 * u0_sqr)) * (1 - R(mu, nu).sum() / 3)\n",
    "    return S\n",
    "\n",
    "def chain_matmul_einsum(arrs, trace_last=True):\n",
    "    # Sequentially multiply the list of matrices.\n",
    "    result = arrs[0]\n",
    "    for A in arrs[1:]:\n",
    "        result = torch.matmul(result, A)\n",
    "    if trace_last:\n",
    "        # Take the trace over the last two dimensions.\n",
    "        result = result.diagonal(offset=0, dim1=-2, dim2=-1).sum(-1)\n",
    "    return result\n",
    "\n",
    "def mean_wilson_rectangle(field, R, T, time_unique=True):\n",
    "    result = 0\n",
    "    if time_unique:\n",
    "        # Loop over spatial directions 1,2,3 (assuming axis 0 is time)\n",
    "        for spatial_dim in [1, 2, 3]:\n",
    "            link_list = []\n",
    "            for i in range(R):\n",
    "                link_list.append(torch.roll(field[:, :, :, :, spatial_dim], shifts=-i, dims=spatial_dim))\n",
    "            for i in range(T):\n",
    "                link_list.append(torch.roll(field[:, :, :, :, 0], shifts=(-i, -R), dims=(0, spatial_dim)))\n",
    "            for i in range(R - 1, -1, -1):\n",
    "                link_list.append(\n",
    "                    torch.roll(field[:, :, :, :, spatial_dim], shifts=(-T, -i), dims=(0, spatial_dim))\n",
    "                         .transpose(-2, -1).conj()\n",
    "                )\n",
    "            for i in range(T - 1, -1, -1):\n",
    "                link_list.append(\n",
    "                    torch.roll(field[:, :, :, :, 0], shifts=-i, dims=0)\n",
    "                         .transpose(-2, -1).conj()\n",
    "                )\n",
    "            result += chain_matmul_einsum(link_list, trace_last=True).mean()\n",
    "        return result / 3\n",
    "    else:\n",
    "        for mu, nu in [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]:\n",
    "            link_list = []\n",
    "            for i in range(R):\n",
    "                link_list.append(torch.roll(field[:, :, :, :, nu], shifts=-i, dims=nu))\n",
    "            for i in range(T):\n",
    "                link_list.append(torch.roll(field[:, :, :, :, mu], shifts=(-i, -R), dims=(mu, nu)))\n",
    "            for i in range(R - 1, -1, -1):\n",
    "                link_list.append(\n",
    "                    torch.roll(field[:, :, :, :, nu], shifts=(-T, -i), dims=(mu, nu))\n",
    "                         .transpose(-2, -1).conj()\n",
    "                )\n",
    "            for i in range(T - 1, -1, -1):\n",
    "                link_list.append(\n",
    "                    torch.roll(field[:, :, :, :, mu], shifts=-i, dims=mu)\n",
    "                         .transpose(-2, -1).conj()\n",
    "                )\n",
    "            result += chain_matmul_einsum(link_list, trace_last=True).mean()\n",
    "        return result / 6\n",
    "\n",
    "def calculate_wilson_loops(gauge_coef, R_range, T_range):\n",
    "    R_min, R_max = R_range\n",
    "    T_min, T_max = T_range\n",
    "    gauge = coef_to_lie_group(gauge_coef)\n",
    "    loops = []\n",
    "    for R, T in itertools.product(range(R_min, R_max + 1), range(T_min, T_max + 1)):\n",
    "        loops.append(mean_wilson_rectangle(gauge, R, T, time_unique=False))\n",
    "    wilson_loop_values = torch.tensor(loops).reshape(R_max - R_min + 1, T_max - T_min + 1)\n",
    "    return wilson_loop_values\n",
    "\n",
    "# --- HMC trajectory and dual-averaging functions ---\n",
    "\n",
    "def parametrized_action(coef, beta):\n",
    "    gauge = coef_to_lie_group(coef)\n",
    "    return wilson_action(gauge, beta)\n",
    "\n",
    "def parametrized_action_grad(coef, beta):\n",
    "    coef.requires_grad_(True)\n",
    "    U = parametrized_action(coef, beta)\n",
    "    # Compute gradient with respect to coef.\n",
    "    grad, = torch.autograd.grad(U, coef, create_graph=True)\n",
    "    return grad\n",
    "\n",
    "def parametrized_action_value_and_grad(coef, beta):\n",
    "    coef.requires_grad_(True)\n",
    "    U = parametrized_action(coef, beta)\n",
    "    grad, = torch.autograd.grad(U, coef, create_graph=True)\n",
    "    return U, grad\n",
    "\n",
    "# --- Compile (optimize) the key functions using torch.compile ---\n",
    "parametrized_action = torch.compile(parametrized_action)\n",
    "parametrized_action_grad = torch.compile(parametrized_action_grad)\n",
    "parametrized_action_value_and_grad = torch.compile(parametrized_action_value_and_grad)\n",
    "\n",
    "def dual_averaging_update(log_eps_bar, h, step, accept_prob, mu, target_accept=0.75, gamma=0.05, t0=10, kappa=0.75):\n",
    "    step = step + 1\n",
    "    eta = 1.0 / (step + t0)\n",
    "    h = (1 - eta) * h + eta * (target_accept - accept_prob)\n",
    "    log_eps = mu - (step**0.5 / gamma) * h\n",
    "    log_eps_bar = (step ** (-kappa)) * log_eps + (1 - step ** (-kappa)) * log_eps_bar\n",
    "    new_epsilon = torch.exp(log_eps) if isinstance(log_eps, torch.Tensor) else np.exp(log_eps)\n",
    "    return new_epsilon, log_eps_bar, h\n",
    "\n",
    "def HMC_trajectory(q0, beta, steps, dt, rng=None):\n",
    "    # Use torch’s random generator if none provided.\n",
    "    if rng is None:\n",
    "        rng = torch.Generator()\n",
    "    p0 = torch.randn_like(q0)\n",
    "    # fixed leapfrog integration\n",
    "    q = q0.clone().detach().requires_grad_(True)\n",
    "    U_initial, grad = parametrized_action_value_and_grad(q, beta)\n",
    "    H_initial = U_initial + torch.sum(p0 ** 2) / 2\n",
    "    # initial half step for momentum\n",
    "    p = p0 + (dt / 2) * grad\n",
    "    for _ in range(steps):\n",
    "        # Full step for q.\n",
    "        q = q + dt * p\n",
    "        U, grad = parametrized_action_value_and_grad(q, beta)\n",
    "        # Full step for p.\n",
    "        p = p - dt * grad\n",
    "    # Final half step for momentum.\n",
    "    p = p - (dt / 2) * grad\n",
    "    U_final = parametrized_action(q, beta)\n",
    "    H_final = U_final + torch.sum(p ** 2) / 2\n",
    "    dH = H_final - H_initial\n",
    "    accept_prob = min(1.0, torch.exp(-dH).item())\n",
    "    # Metropolis acceptance step.\n",
    "    if torch.rand(1).item() < accept_prob:\n",
    "        q_next = q.detach()\n",
    "        accepted = True\n",
    "    else:\n",
    "        q_next = q0\n",
    "        accepted = False\n",
    "    return {\n",
    "        \"q_next\": q_next,\n",
    "        \"dH\": dH,\n",
    "        \"accept_prob\": accept_prob,\n",
    "        \"was_accepted\": accepted\n",
    "    }\n",
    "\n",
    "def warmup_epsilon(coef, beta, target_accept=0.651, eps0=1e-3, warmup_iters=300, trajectory_steps=30):\n",
    "    import math\n",
    "    from tqdm import tqdm\n",
    "    mu = math.log(10 * eps0)\n",
    "    log_eps_bar = 0.0\n",
    "    h = 0.0\n",
    "    eps = eps0\n",
    "    for step in tqdm(range(warmup_iters), desc=\"Warmup epsilon\"):\n",
    "        traj = HMC_trajectory(coef, beta, trajectory_steps, eps)\n",
    "        coef = traj[\"q_next\"]\n",
    "        accept_prob = traj[\"accept_prob\"]\n",
    "        print(f\"warmup step {step} : eps={eps} ; p={accept_prob} ; dH={traj['dH']}\")\n",
    "        eps, log_eps_bar, h = dual_averaging_update(log_eps_bar, h, step, accept_prob, mu, target_accept=target_accept)\n",
    "    return coef, eps\n",
    "\n",
    "def warmup_tint(coef, beta, observable_fn, eps, warmup_iters=2000, trajectory_steps=30):\n",
    "    from tqdm import tqdm\n",
    "    O_list = []\n",
    "    for step in tqdm(range(warmup_iters), desc=\"Warmup tint\"):\n",
    "        traj = HMC_trajectory(coef, beta, trajectory_steps, eps)\n",
    "        coef = traj[\"q_next\"]\n",
    "        o = observable_fn(coef)\n",
    "        print(f\"warmup step {step} ; o={o}\")\n",
    "        O_list.append(o)\n",
    "    O = torch.stack(O_list)\n",
    "    O = O.view(-1)\n",
    "    tint = 1.0\n",
    "    for s in range(1, O.shape[0] // 2):\n",
    "        # Using torch.corrcoef (available in recent PyTorch releases)\n",
    "        corr = torch.corrcoef(torch.stack([O[:-s], O[s:]]))[0, 1]\n",
    "        tint += 2 * corr\n",
    "    return coef, tint, O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "L = (16, 8, 8, 8)\n",
    "\n",
    "coef = torch.normal(0, 1, (*L, 4, 8), dtype=torch.float32, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup epsilon:   0%|          | 0/300 [00:00<?, ?it/s]E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1] failed while attempting to run meta for aten.view.dtype\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1] Traceback (most recent call last):\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1]   File \"/home/nobe/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2384, in _dispatch_impl\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1]     r = func(*args, **kwargs)\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1]   File \"/home/nobe/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_ops.py\", line 723, in __call__\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1]     return self._op(*args, **kwargs)\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E0306 20:21:20.526000 19306 site-packages/torch/_subclasses/fake_tensor.py:2388] [0/0_1] RuntimeError: self.stride(-1) must be 1 to view ComplexFloat as Float (different element sizes), but got 3\n",
      "/home/nobe/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/autograd/graph.py:823: UserWarning: Error detected in RollBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/tmp/ipykernel_19306/105200041.py\", line 159, in parametrized_action_value_and_grad\n",
      "    U = parametrized_action(coef, beta)\n",
      "  File \"/home/nobe/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py\", line 160, in getattr_and_trace\n",
      "    return fn(*args[2:], **kwargs)\n",
      "  File \"/tmp/ipykernel_19306/105200041.py\", line 148, in parametrized_action\n",
      "    return wilson_action(gauge, beta)\n",
      "  File \"/tmp/ipykernel_19306/105200041.py\", line 33, in wilson_action\n",
      "    S += torch.sum(plaquette(mu, nu))\n",
      "  File \"/tmp/ipykernel_19306/105200041.py\", line 21, in plaquette\n",
      "    U_nu_shifted = torch.roll(U_nu, shifts=-1, dims=mu)\n",
      " (Triggered internally at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:122.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Warmup epsilon:   0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: self.stride(-1) must be 1 to view ComplexFloat as Float (different element sizes), but got 3\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBackendCompilerFailed\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m coef, eps = \u001b[43mwarmup_epsilon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrajectory_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mwarmup_epsilon\u001b[39m\u001b[34m(coef, beta, target_accept, eps0, warmup_iters, trajectory_steps)\u001b[39m\n\u001b[32m    220\u001b[39m eps = eps0\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(warmup_iters), desc=\u001b[33m\"\u001b[39m\u001b[33mWarmup epsilon\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     traj = \u001b[43mHMC_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrajectory_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     coef = traj[\u001b[33m\"\u001b[39m\u001b[33mq_next\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    224\u001b[39m     accept_prob = traj[\u001b[33m\"\u001b[39m\u001b[33maccept_prob\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mHMC_trajectory\u001b[39m\u001b[34m(q0, beta, steps, dt, rng)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# fixed leapfrog integration\u001b[39;00m\n\u001b[32m    183\u001b[39m q = q0.clone().detach().requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m U_initial, grad = \u001b[43mparametrized_action_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m H_initial = U_initial + torch.sum(p0 ** \u001b[32m2\u001b[39m) / \u001b[32m2\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# initial half step for momentum\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:574\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m saved_dynamic_layer_stack_depth = (\n\u001b[32m    570\u001b[39m     torch._C._functorch.get_dynamic_layer_stack_depth()\n\u001b[32m    571\u001b[39m )\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    576\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    577\u001b[39m     torch._C._functorch.pop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[32m    578\u001b[39m         saved_dynamic_layer_stack_depth\n\u001b[32m    579\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1380\u001b[39m, in \u001b[36mCatchErrorsWrapper.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, frame_state)\u001b[39m\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[32m   1375\u001b[39m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m.hooks, frame_state\n\u001b[32m   1376\u001b[39m             )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[32m   1379\u001b[39m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1164\u001b[39m, in \u001b[36mConvertFrame.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m   1162\u001b[39m counters[\u001b[33m\"\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m   1163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     counters[\u001b[33m\"\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:547\u001b[39m, in \u001b[36mConvertFrameAssert.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m    544\u001b[39m     dynamo_tls.traced_frame_infos.append(info)\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:986\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[39m\n\u001b[32m    984\u001b[39m guarded_code = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m986\u001b[39m     guarded_code = \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[32m    989\u001b[39m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    995\u001b[39m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[32m    996\u001b[39m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[32m    997\u001b[39m     put_code_state()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:715\u001b[39m, in \u001b[36m_compile.<locals>.compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    713\u001b[39m     stack.enter_context(torch._dynamo.callback_handler.install_callbacks())\n\u001b[32m    714\u001b[39m     stack.enter_context(CompileTimeInstructionCounter.record())\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_utils_internal.py:95\u001b[39m, in \u001b[36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m] = skip + \u001b[32m1\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler.enabled:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler.profile_compile_time(\n\u001b[32m     98\u001b[39m     function, phase_name, *args, **kwargs\n\u001b[32m     99\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:750\u001b[39m, in \u001b[36m_compile.<locals>._compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    748\u001b[39m CompileContext.get().attempt = attempt\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     out_code = \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.RestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1361\u001b[39m, in \u001b[36mtransform_code_object\u001b[39m\u001b[34m(code, transformations, safe)\u001b[39m\n\u001b[32m   1358\u001b[39m instructions = cleaned_instructions(code, safe)\n\u001b[32m   1359\u001b[39m propagate_line_nums(instructions)\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:231\u001b[39m, in \u001b[36mpreserve_global_state.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m exit_stack.enter_context(torch_function_mode_stack_state_mgr)\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    233\u001b[39m     cleanup.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:662\u001b[39m, in \u001b[36m_compile.<locals>.transform\u001b[39m\u001b[34m(instructions, code_options)\u001b[39m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    661\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer.output.tracing_context), tracer.set_current_tx():\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.UnspecializeRestartAnalysis:\n\u001b[32m    664\u001b[39m     speculation_log.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2868\u001b[39m, in \u001b[36mInstructionTranslator.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2867\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2868\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1052\u001b[39m, in \u001b[36mInstructionTranslatorBase.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28mself\u001b[39m.output.push_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1053\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:962\u001b[39m, in \u001b[36mInstructionTranslatorBase.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28mself\u001b[39m.update_block_stack(inst)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output.should_exit\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:657\u001b[39m, in \u001b[36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m speculation.failed:\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m speculation.reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_graph_break\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeculation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreason\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:698\u001b[39m, in \u001b[36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.handle_graph_break\u001b[39m\u001b[34m(self, inst, reason)\u001b[39m\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_graph_break\u001b[39m(\n\u001b[32m    694\u001b[39m     \u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mInstructionTranslatorBase\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    695\u001b[39m     inst: Instruction,\n\u001b[32m    696\u001b[39m     reason: GraphCompileReason,\n\u001b[32m    697\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreason\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreason\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m     cg = PyCodegen(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    700\u001b[39m     cleanup: List[Instruction] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1136\u001b[39m, in \u001b[36mOutputGraph.compile_subgraph\u001b[39m\u001b[34m(self, tx, partial_convert, reason)\u001b[39m\n\u001b[32m   1133\u001b[39m output = []\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m.graph) != \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2.graph_outputs) != \u001b[32m0\u001b[39m:\n\u001b[32m   1135\u001b[39m     output.extend(\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgraph_output_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_replacements\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     )\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2.graph_outputs) != \u001b[32m0\u001b[39m:\n\u001b[32m   1142\u001b[39m         output.append(pass2.create_store(graph_output_var))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1382\u001b[39m, in \u001b[36mOutputGraph.compile_and_call_fx_graph\u001b[39m\u001b[34m(self, tx, rv, root, replaced_outputs)\u001b[39m\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracing_context.fake_mode = backend_fake_mode\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.restore_global_state():\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     compiled_fn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[32m   1386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1387\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[33m\"\u001b[39m\u001b[33m__self__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m_lazy_forward\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1392\u001b[39m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[32m   1393\u001b[39m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1432\u001b[39m, in \u001b[36mOutputGraph.call_user_compiler\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx.GraphModule) -> CompiledFn:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   1427\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOutputGraph.call_user_compiler\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1428\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mbackend_compile\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1429\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1430\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33maot_autograd_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1431\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1483\u001b[39m, in \u001b[36mOutputGraph._call_user_compiler\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m.compiler_fn, e).with_traceback(\n\u001b[32m   1484\u001b[39m         e.__traceback__\n\u001b[32m   1485\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m signpost_event(\n\u001b[32m   1488\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdynamo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1489\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOutputGraph.call_user_compiler\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1495\u001b[39m     },\n\u001b[32m   1496\u001b[39m )\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1462\u001b[39m, in \u001b[36mOutputGraph._call_user_compiler\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.verify_correctness:\n\u001b[32m   1461\u001b[39m     compiler_fn = WrapperBackend(compiler_fn)\n\u001b[32m-> \u001b[39m\u001b[32m1462\u001b[39m compiled_fn = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1463\u001b[39m _step_logger()(logging.INFO, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1464\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[33m\"\u001b[39m\u001b[33mcompiler_fn did not return callable\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:130\u001b[39m, in \u001b[36mWrapBackendDebug.__call__\u001b[39m\u001b[34m(self, gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     compiled_gm = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/__init__.py:2340\u001b[39m, in \u001b[36m_TorchCompileInductorWrapper.__call__\u001b[39m\u001b[34m(self, model_, inputs_)\u001b[39m\n\u001b[32m   2337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[32m   2338\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1863\u001b[39m, in \u001b[36mcompile_fx\u001b[39m\u001b[34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[39m\n\u001b[32m   1856\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m V.set_fake_mode(fake_mode), torch._guards.tracing(\n\u001b[32m   1859\u001b[39m     tracing_context\n\u001b[32m   1860\u001b[39m ), compiled_autograd._disable(), functorch_config.patch(\n\u001b[32m   1861\u001b[39m     unlift_effect_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1862\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1863\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1866\u001b[39m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1871\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:83\u001b[39m, in \u001b[36mAotAutograd.__call__\u001b[39m\u001b[34m(self, gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         cg = \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m         counters[\u001b[33m\"\u001b[39m\u001b[33maot_autograd\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1155\u001b[39m, in \u001b[36maot_module_simplified\u001b[39m\u001b[34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[39m\n\u001b[32m   1145\u001b[39m     compiled_fn = AOTAutogradCache.load(\n\u001b[32m   1146\u001b[39m         dispatch_and_compile,\n\u001b[32m   1147\u001b[39m         mod,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1152\u001b[39m         remote,\n\u001b[32m   1153\u001b[39m     )\n\u001b[32m   1154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1155\u001b[39m     compiled_fn = \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch._dynamo.utils.GmWrapper):\n\u001b[32m   1158\u001b[39m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[32m   1159\u001b[39m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[32m   1160\u001b[39m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[32m   1161\u001b[39m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[32m   1162\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1131\u001b[39m, in \u001b[36maot_module_simplified.<locals>.dispatch_and_compile\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1129\u001b[39m functional_call = create_functional_call(mod, params_spec, params_len)\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd._disable():\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     compiled_fn, _ = \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:580\u001b[39m, in \u001b[36mcreate_aot_dispatcher_function\u001b[39m\u001b[34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[32m    573\u001b[39m     flat_fn,\n\u001b[32m    574\u001b[39m     fake_flat_args: FakifiedFlatArgs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    577\u001b[39m     shape_env: Optional[ShapeEnv],\n\u001b[32m    578\u001b[39m ) -> Tuple[Callable, ViewAndMutationMeta]:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mcreate_aot_dispatcher_function\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:830\u001b[39m, in \u001b[36m_create_aot_dispatcher_function\u001b[39m\u001b[34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[39m\n\u001b[32m    826\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[32m    828\u001b[39m compiler_fn = choose_dispatcher(needs_autograd, aot_config)\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m compiled_fn, fw_metadata = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:396\u001b[39m, in \u001b[36maot_dispatch_autograd\u001b[39m\u001b[34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[39m\n\u001b[32m    387\u001b[39m flat_fn, flat_args, fw_metadata = pre_compile(\n\u001b[32m    388\u001b[39m     wrappers,\n\u001b[32m    389\u001b[39m     flat_fn,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     fw_metadata=fw_metadata,\n\u001b[32m    393\u001b[39m )\n\u001b[32m    395\u001b[39m fw_metadata.deterministic = torch.are_deterministic_algorithms_enabled()\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m fx_g, joint_inputs, maybe_subclass_meta = \u001b[43maot_dispatch_autograd_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfw_metadata\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# Copied from aot_dispatch_autograd_graph.\u001b[39;00m\n\u001b[32m    401\u001b[39m disable_amp = torch._C._is_any_autocast_enabled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:318\u001b[39m, in \u001b[36maot_dispatch_autograd_graph\u001b[39m\u001b[34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[39m\n\u001b[32m    313\u001b[39m     saved_updated_joint_inputs = pytree.tree_map_only(\n\u001b[32m    314\u001b[39m         torch.Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m t: t.detach(), updated_joint_inputs\n\u001b[32m    315\u001b[39m     )\n\u001b[32m    316\u001b[39m maybe_subclass_meta = subclass_tracing_info.maybe_subclass_meta\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m fx_g = \u001b[43m_create_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoint_fn_to_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_joint_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# There should be *NO* mutating ops in the graph at this point.\u001b[39;00m\n\u001b[32m    321\u001b[39m assert_functional_graph(fx_g.graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py:55\u001b[39m, in \u001b[36m_create_graph\u001b[39m\u001b[34m(f, args, aot_config)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_graph\u001b[39m(f, args, *, aot_config: AOTConfig) -> torch.fx.GraphModule:\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# FunctionalTensorMode must be enabled here.\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# See Note [Accessing .grad_fn on FunctionalTensor]\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m enable_python_dispatcher(), FunctionalTensorMode(\n\u001b[32m     50\u001b[39m         pre_dispatch=aot_config.pre_dispatch,\n\u001b[32m     51\u001b[39m         export=aot_config.is_export,\n\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# Allow token discovery for joint fn tracing as tokens can be used in backward.\u001b[39;00m\n\u001b[32m     53\u001b[39m         _allow_token_discovery=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     54\u001b[39m     ):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         fx_g = \u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecomposition_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrecord_module_stack\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fx_g\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:2196\u001b[39m, in \u001b[36mmake_fx.<locals>.wrapped\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m   2194\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m   2195\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args: \u001b[38;5;28mobject\u001b[39m) -> GraphModule:\n\u001b[32m-> \u001b[39m\u001b[32m2196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_fx_tracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:2134\u001b[39m, in \u001b[36m_MakefxTracer.trace\u001b[39m\u001b[34m(self, f, *args)\u001b[39m\n\u001b[32m   2132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrace\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable, *args: \u001b[38;5;28mobject\u001b[39m) -> fx.GraphModule:\n\u001b[32m   2133\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_modes_from_inputs(f, args):\n\u001b[32m-> \u001b[39m\u001b[32m2134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trace_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:2105\u001b[39m, in \u001b[36m_MakefxTracer._trace_inner\u001b[39m\u001b[34m(self, f, *args)\u001b[39m\n\u001b[32m   2103\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fx_tracer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2105\u001b[39m     t = \u001b[43mdispatch_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2110\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   2111\u001b[39m     trace_structured(\n\u001b[32m   2112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33martifact\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2113\u001b[39m         metadata_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m   2122\u001b[39m         ).src,\n\u001b[32m   2123\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:1138\u001b[39m, in \u001b[36mdispatch_trace\u001b[39m\u001b[34m(root, tracer, concrete_args)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdispatch_trace\u001b[39m(\n\u001b[32m   1134\u001b[39m     root: Union[Module, Callable],\n\u001b[32m   1135\u001b[39m     tracer: Tracer,\n\u001b[32m   1136\u001b[39m     concrete_args: Optional[Tuple[Any, ...]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1137\u001b[39m ) -> GraphModule:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m     graph = \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;66;03m# NB: be careful not to DCE .item() calls\u001b[39;00m\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpure_pred\u001b[39m(n: fx.Node) -> \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py:843\u001b[39m, in \u001b[36mTracer.trace\u001b[39m\u001b[34m(self, root, concrete_args)\u001b[39m\n\u001b[32m    836\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._autowrap_search:\n\u001b[32m    837\u001b[39m             _autowrap_check(\n\u001b[32m    838\u001b[39m                 patcher, module.\u001b[34m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m._autowrap_function_ids\n\u001b[32m    839\u001b[39m             )\n\u001b[32m    840\u001b[39m         \u001b[38;5;28mself\u001b[39m.create_node(\n\u001b[32m    841\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    842\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m             (\u001b[38;5;28mself\u001b[39m.create_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[32m    844\u001b[39m             {},\n\u001b[32m    845\u001b[39m             type_expr=fn.\u001b[34m__annotations__\u001b[39m.get(\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    846\u001b[39m         )\n\u001b[32m    848\u001b[39m     \u001b[38;5;28mself\u001b[39m.submodule_paths = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py:700\u001b[39m, in \u001b[36mTracer.create_args_for_root.<locals>.flatten_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_fn\u001b[39m(*args):\n\u001b[32m    699\u001b[39m     tree_args = pytree.tree_unflatten(\u001b[38;5;28mlist\u001b[39m(args), in_spec)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     tree_out = \u001b[43mroot_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtree_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m     out_args, out_spec = pytree.tree_flatten(tree_out)\n\u001b[32m    702\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.graph._codegen, _PyTreeCodeGen)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:1193\u001b[39m, in \u001b[36mwrap_key.<locals>.wrapped\u001b[39m\u001b[34m(*proxies, **_unused)\u001b[39m\n\u001b[32m   1190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tensor_proxy_slot\u001b[39m(t: Tensor) -> Union[Tensor, Proxy]:\n\u001b[32m   1191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x.proxy)\n\u001b[32m-> \u001b[39m\u001b[32m1193\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type:ignore[call-arg]\u001b[39;00m\n\u001b[32m   1194\u001b[39m out = pytree.tree_map_only(Tensor, get_tensor_proxy_slot, out)\n\u001b[32m   1195\u001b[39m out = pytree.tree_map_only(\n\u001b[32m   1196\u001b[39m     _AnyScriptObject, \u001b[38;5;28;01mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x), out\n\u001b[32m   1197\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:693\u001b[39m, in \u001b[36mhandle_effect_tokens_fn.<locals>.inner_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    690\u001b[39m         functional_tensor_mode._tokens[k] = f_tokens[i]\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# Run the joint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m     outs = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# Return both the tokens and the outputs\u001b[39;00m\n\u001b[32m    696\u001b[39m \u001b[38;5;66;03m# See Note [Side-Effectful Tokens in AOTAutograd]\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trace_joint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:644\u001b[39m, in \u001b[36mcreate_functionalized_fn.<locals>.joint_helper\u001b[39m\u001b[34m(primals, tangents)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjoint_helper\u001b[39m(primals, tangents):\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_functionalized_f_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:413\u001b[39m, in \u001b[36mcreate_functionalized_fn.<locals>._functionalized_f_helper\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    410\u001b[39m     f_args = pytree.tree_map(to_fun, args)\n\u001b[32m    412\u001b[39m     \u001b[38;5;66;03m# Run the joint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     f_outs = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mf_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trace_joint:\n\u001b[32m    416\u001b[39m     \u001b[38;5;66;03m# We support a limited amount of mutation of graph inputs during the backward pass.\u001b[39;00m\n\u001b[32m    417\u001b[39m     \u001b[38;5;66;03m# (This is used e.g. by Float8, which needs to update buffers during the backward pass)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    425\u001b[39m     \u001b[38;5;66;03m#   the bw by running our analysis first on the fw-only graph, and then on the joint graph. This would\u001b[39;00m\n\u001b[32m    426\u001b[39m     \u001b[38;5;66;03m#   require an extra round of tracing though, so it's more efficient to do in-line here.\u001b[39;00m\n\u001b[32m    427\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    428\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[32m    429\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) == \u001b[32m2\u001b[39m\n\u001b[32m    430\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[32m    431\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:280\u001b[39m, in \u001b[36mcreate_joint.<locals>.inner_fn_with_anomaly\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    278\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAnomaly Detection has been enabled.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.detect_anomaly(check_nan=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py:265\u001b[39m, in \u001b[36mcreate_joint.<locals>.inner_fn\u001b[39m\u001b[34m(primals, tangents)\u001b[39m\n\u001b[32m    259\u001b[39m             backward_out = torch.autograd.grad(\n\u001b[32m    260\u001b[39m                 needed_outs,\n\u001b[32m    261\u001b[39m                 grad_primals,\n\u001b[32m    262\u001b[39m                 allow_unused=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    263\u001b[39m             )\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m             backward_out = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m                \u001b[49m\u001b[43mneeded_outs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m                \u001b[49m\u001b[43mgrad_primals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m                \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneeded_tangents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m                \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m backward_out_iter = \u001b[38;5;28miter\u001b[39m(backward_out)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, [\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mnext\u001b[39m(backward_out_iter) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs_needs_grads\n\u001b[32m    274\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/autograd/__init__.py:445\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    443\u001b[39m overridable_args = t_outputs + t_inputs\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverridable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_inputs:\n\u001b[32m    460\u001b[39m     warnings.warn(\n\u001b[32m    461\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    465\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    466\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/overrides.py:1720\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1717\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1718\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1719\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1720\u001b[39m         result = \u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1721\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1722\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:1241\u001b[39m, in \u001b[36mTorchFunctionMetadataMode.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28mself\u001b[39m.tracer.torch_fn_metadata = func\n\u001b[32m   1240\u001b[39m \u001b[38;5;28mself\u001b[39m.tracer.torch_fn_counts[func] = \u001b[38;5;28mself\u001b[39m.tracer.torch_fn_counts.get(func, \u001b[32m0\u001b[39m) + \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:527\u001b[39m, in \u001b[36mFunctionalTensorMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m     outs_wrapped = pytree.tree_map_only(\n\u001b[32m    519\u001b[39m         torch.Tensor, wrap, outs_unwrapped\n\u001b[32m    520\u001b[39m     )\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    522\u001b[39m     \u001b[38;5;66;03m# When we dispatch to the C++ functionalization kernel, we might need to jump back to the\u001b[39;00m\n\u001b[32m    523\u001b[39m     \u001b[38;5;66;03m# PreDispatch mode stack afterwards, to handle any other PreDispatch modes underneath\u001b[39;00m\n\u001b[32m    524\u001b[39m     \u001b[38;5;66;03m# FunctionalTensorMode. If we call func() directly, we would need to exclude PreDispatch\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;66;03m# from the TLS in order to avoid infinite looping, but this would prevent us from coming\u001b[39;00m\n\u001b[32m    526\u001b[39m     \u001b[38;5;66;03m# back to PreDispatch later\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     outs_unwrapped = \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_op_dk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDispatchKey\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFunctionalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_unwrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m     \u001b[38;5;66;03m# We don't allow any mutation on result of dropout or _to_copy\u001b[39;00m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.export:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/utils/_stats.py:21\u001b[39m, in \u001b[36mcount.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m     simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m     20\u001b[39m simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:1343\u001b[39m, in \u001b[36mProxyTorchDispatchMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m (prim.device.default,):\n\u001b[32m   1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproxy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:789\u001b[39m, in \u001b[36mproxy_call\u001b[39m\u001b[34m(proxy_mode, func, pre_dispatch, args, kwargs)\u001b[39m\n\u001b[32m    783\u001b[39m     not_implemented_log.debug(\n\u001b[32m    784\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mProxyTensorMode tensors without proxy had unrecognized subclasses: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    785\u001b[39m         unrecognized_types,\n\u001b[32m    786\u001b[39m     )\n\u001b[32m    787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m r = \u001b[43mmaybe_handle_decomp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    791\u001b[39m     _maybe_record_pointwise_barrier(func, proxy_mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:2264\u001b[39m, in \u001b[36mmaybe_handle_decomp\u001b[39m\u001b[34m(proxy_mode, op, args, kwargs)\u001b[39m\n\u001b[32m   2262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m proxy_mode:\n\u001b[32m   2263\u001b[39m     proxy_mode.decomp_layers += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2264\u001b[39m     out = \u001b[43mCURRENT_DECOMPOSITION_TABLE\u001b[49m\u001b[43m[\u001b[49m\u001b[43mop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2265\u001b[39m     proxy_mode.decomp_layers -= \u001b[32m1\u001b[39m\n\u001b[32m   2266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_inductor/decomposition.py:451\u001b[39m, in \u001b[36madd\u001b[39m\u001b[34m(x, y, alpha)\u001b[39m\n\u001b[32m    448\u001b[39m     reshaped_tensor = tensor.view(new_shape)\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reshaped_tensor\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m x_reshaped = reshape_tensor_complex(\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    452\u001b[39m z_reshaped = reshape_tensor_complex(z.view(y.real.dtype))\n\u001b[32m    453\u001b[39m result = torch.flatten(x_reshaped + z_reshaped, start_dim=-\u001b[32m2\u001b[39m).view(complex_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/utils/_stats.py:21\u001b[39m, in \u001b[36mcount.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m     simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m     20\u001b[39m simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:1343\u001b[39m, in \u001b[36mProxyTorchDispatchMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m (prim.device.default,):\n\u001b[32m   1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproxy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py:912\u001b[39m, in \u001b[36mproxy_call\u001b[39m\u001b[34m(proxy_mode, func, pre_dispatch, args, kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m proxy_out = proxy_mode.tracer.create_proxy(\n\u001b[32m    904\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcall_function\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    905\u001b[39m     func,\n\u001b[32m   (...)\u001b[39m\u001b[32m    908\u001b[39m     name=proxy_mode.tracer.graph._target_to_str(func.overloadpacket.\u001b[34m__name__\u001b[39m),\n\u001b[32m    909\u001b[39m )\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _enable_thunkify(proxy_mode.tracer):\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[38;5;66;03m# In some circumstances, we will be tracing in a situation where a tensor\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[38;5;66;03m# is *statically* known to be a constant (currently, this only happens if\u001b[39;00m\n\u001b[32m    916\u001b[39m \u001b[38;5;66;03m# you run torch.tensor; deterministic factory functions like torch.arange\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# propagating const-ness.  Similarly, we don't require the constant to\u001b[39;00m\n\u001b[32m    934\u001b[39m \u001b[38;5;66;03m# live on CPU, but we could.\u001b[39;00m\n\u001b[32m    935\u001b[39m any_constant = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    936\u001b[39m     t.constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    937\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m f_flat_args_kwargs\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, _ProxyTensor)\n\u001b[32m    939\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_ops.py:723\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/utils/_stats.py:21\u001b[39m, in \u001b[36mcount.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m     simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m     20\u001b[39m simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1276\u001b[39m, in \u001b[36mFakeTensorMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m   1273\u001b[39m     torch._C._get_dispatch_mode(torch._C._TorchDispatchModeKey.FAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1274\u001b[39m ), func\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     log.exception(\u001b[33m\"\u001b[39m\u001b[33mfake tensor raised TypeError\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1816\u001b[39m, in \u001b[36mFakeTensorMode.dispatch\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1813\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m   1815\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_enabled:\n\u001b[32m-> \u001b[39m\u001b[32m1816\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dispatch_impl(func, types, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1377\u001b[39m, in \u001b[36mFakeTensorMode._cached_dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1376\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_cache_key(func, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m     entry = \u001b[38;5;28mself\u001b[39m._make_cache_entry(state, key, func, args, kwargs, output)\n\u001b[32m   1379\u001b[39m     key.strip_shape_env()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2384\u001b[39m, in \u001b[36mFakeTensorMode._dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   2382\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2383\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m in_kernel_invocation_manager(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2384\u001b[39m         r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m not_implemented_error:\n\u001b[32m   2386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m maybe_run_unsafe_fallback(not_implemented_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lattice-qcd/lib/python3.11/site-packages/torch/_ops.py:723\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mBackendCompilerFailed\u001b[39m: backend='inductor' raised:\nRuntimeError: self.stride(-1) must be 1 to view ComplexFloat as Float (different element sizes), but got 3\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "coef, eps = warmup_epsilon(\n",
    "    coef,\n",
    "    beta=6.7,\n",
    "    target_accept=0.4,\n",
    "    eps0=1e-3,\n",
    "    warmup_iters=300,\n",
    "    trajectory_steps=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lattice-qcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
